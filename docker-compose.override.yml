services:
# USE LIBRECHAT CONFIG FILE
  api:
    depends_on:
      mongodb:
        condition: service_started
      rag_api:
        condition: service_started
      ollama:
        condition: service_started
    environment:
      - NODE_ENV=development # development mode allows http cookies for login
      - ALLOW_DEV_COOKIES=true # Required: npm script uses cross-env NODE_ENV=production, so this enables lax sameSite cookies for network access
    volumes:
    #- /home/christoph/Repositories/Github/chrsch/plantuml-mcp-server:/app/plantuml-mcp-server
    - type: bind
      source: /home/christoph/Repositories/Github/chrsch/LibreChat/librechat.yaml
      target: /app/librechat.yaml
    - type: bind
      source: /home/christoph/Repositories/Github/chrsch/LibreChat/api/server/services/AuthService.js
      target: /app/api/server/services/AuthService.js
    # Collmex-Invoices MCP server (built dist + node_modules)
    - type: bind
      source: ./mcp-servers/collmex-invoices
      target: /app/mcp-servers/collmex-invoices
      read_only: true
  
  rag_api:
    image: ghcr.io/danny-avila/librechat-rag-api-dev:latest

  # ADD OLLAMA
  ollama:
    container_name: Ollama
    image: ollama/ollama:latest
    restart: always
    ports: ["11434:11434"]
    volumes:
      # Named volume keeps downloaded model blobs/manifests across `compose down/up`.
      # Only `compose down -v` would delete this volume.
      - ollama_models:/root/.ollama/models
      # Modelfile definitions and init script remain as bind mounts (version-controlled).
      - ./ollama/Modelfiles:/root/.ollama/Modelfiles:ro
      - ./ollama/init-ollama.sh:/usr/local/bin/init-ollama.sh:ro
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - OLLAMA_MODELS=${OLLAMA_MODELS}
    gpus: all
    entrypoint: ["/bin/bash", "/usr/local/bin/init-ollama.sh"]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 120s

volumes:
  # Persists Ollama model blobs and manifests across compose down/up cycles.
  # Remove with `docker compose down -v` only when you intentionally want to delete all models.
  ollama_models:

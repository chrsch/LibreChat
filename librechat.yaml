version: 1.2.8
cache: true

# Show the model selector even with modelSpecs, so tools are visible in normal chats
interface:
  modelSelect: true
  parameters: true
  webSearch: true

# Web Search Configuration
webSearch:
  # Search Provider - SearXNG (self-hosted, no API key needed)
  # The instance URL is read from SEARXNG_INSTANCE_URL environment variable
  searxngInstanceUrl: "${SEARXNG_INSTANCE_URL}"
  searchProvider: "searxng"
  
  # Scraper Configuration - Users will provide API key via UI
  # Get your API key from: https://firecrawl.dev
  firecrawlApiKey: "${FIRECRAWL_API_KEY}"
  scraperType: "firecrawl"
  
  # Reranker Configuration - Users will provide API key via UI
  # Get Jina API key from: https://jina.ai/api-dashboard/
  jinaApiKey: "${JINA_API_KEY}"
  rerankerType: "jina"
  
  # General Settings
  scraperTimeout: 7500
  safeSearch: 1  # 0=OFF, 1=MODERATE (default), 2=STRICT

# MCP Servers Configuration
#mcpServers:
#  PlantUML:
#    type: stdio
#    command: npx
#    args:
#      - -y
#      - plantuml-mcp-server # Assumes plantuml-mcp-server is installed/mounted locally, see docker-compose.override.yml
#    env:
#      PLANTUML_SERVER_URL: https://www.plantuml.com/plantuml
#    timeout: 60000
#    initTimeout: 30000
#    serverInstructions: "The generate_plantuml_diagram tool returns an image URL. Display it using markdown: ![Diagram](URL)"

endpoints:
  # Note: Ollama is not supported as a provider for the Agents endpoint in v0.8.0
  # Web search can still be used through the custom Ollama endpoint with tools capability

  custom:
    - name: "Ollama"
      apiKey: "ollama"
      baseURL: "http://host.docker.internal:11434/"
      models:
        default: [
          "llama3.2-3b-local"
          ]
        fetch: true # fetching list of models is not supported
      titleConvo: true
      titleModel: "current_model"
      # Enable tools/actions for web search functionality
      capabilities:
        - tools

      # Theses parameters go to Ollama runtime
      addParams:
        options:
          # Offload & throughput
          gpu_layers: 27

          # Sampling â€“ conservative
          temperature: 0.2
          top_p: 0.9
          top_k: 40
          mirostat: 2
          mirostat_tau: 5.0
          mirostat_eta: 0.1
          repeat_penalty: 1.15
          repeat_last_n: 64

          # Determinism
          seed: 1

          # Optional guard-rails
          stop: ["\n\nUser:", "\nUser:"]